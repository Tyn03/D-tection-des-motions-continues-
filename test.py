import numpy as np
import cv2
import tensorflow as tf
import json
import streamlit as st
import matplotlib.pyplot as plt
from datetime import datetime
from matplotlib import dates as mdates
from PIL import Image
import io
import tempfile
import os
import time
import threading
import queue
from scipy.stats import gaussian_kde
from scipy.ndimage import gaussian_filter
import os
os.environ["STREAMLIT_WATCH_FILE_SYSTEM"] = "false"
import math
import mediapipe as mp

# === Setup ===
TIMEOUT = 30
FRAME_SKIP = 3
LOG_INTERVAL = 5
MAX_LOG_LENGTH = 300
SAVE_INTERVAL = 10  # Save data every 10 seconds
DETECT_EVERY_N_FRAMES = 2  # Ch·ªâ d·ª± ƒëo√°n m·ªói n frame ƒë·ªÉ gi·∫£m t·∫£i cho CPU

# Kh·ªüi t·∫°o MediaPipe
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# Kh·ªüi t·∫°o bi·∫øn to√†n c·ª•c cho theo d√µi chuy·ªÉn ƒë·ªông
prev_head_pose = None
prev_shoulders_pos = None
agitation_window = []

# ƒê·ªãnh nghƒ©a l·∫°i v·ªã tr√≠ c√°c c·∫£m x√∫c tr√™n kh√¥ng gian valence-arousal
# S·ª≠ d·ª•ng c√°c gi√° tr·ªã ƒë√£ ƒë∆∞·ª£c nghi√™n c·ª©u k·ªπ l∆∞·ª°ng h∆°n
emotion_map = {
    # V√πng ph·∫£i tr√™n - c·∫£m x√∫c t√≠ch c·ª±c, k√≠ch th√≠ch cao
    "Happy":     {"valence": 0.8, "arousal": 0.4, "emoji": "üòÑ"},      # H·∫°nh ph√∫c: t√≠ch c·ª±c, ph·∫•n kh√≠ch v·ª´a ph·∫£i
    "Surprise":  {"valence": 0.4, "arousal": 0.8, "emoji": "üò≤"},      # Ng·∫°c nhi√™n: t√≠ch c·ª±c v·ª´a ph·∫£i, k√≠ch th√≠ch cao
    "Excited":   {"valence": 0.7, "arousal": 0.7, "emoji": "ü§©"},      # H√†o h·ª©ng: t√≠ch c·ª±c cao, k√≠ch th√≠ch cao
    
    # V√πng tr√°i tr√™n - c·∫£m x√∫c ti√™u c·ª±c, k√≠ch th√≠ch cao
    "Angry":     {"valence": -0.7, "arousal": 0.6, "emoji": "üò†"},     # Gi·∫≠n d·ªØ: ti√™u c·ª±c cao, k√≠ch th√≠ch cao
    "Fear":      {"valence": -0.5, "arousal": 0.7, "emoji": "üò®"},     # S·ª£ h√£i: ti√™u c·ª±c, k√≠ch th√≠ch cao
    "Disgust":   {"valence": -0.6, "arousal": 0.3, "emoji": "ü§¢"},     # Gh√™ t·ªüm: ti√™u c·ª±c, k√≠ch th√≠ch v·ª´a ph·∫£i
    
    # V√πng tr√°i d∆∞·ªõi - c·∫£m x√∫c ti√™u c·ª±c, k√≠ch th√≠ch th·∫•p
    "Sad":       {"valence": -0.6, "arousal": -0.3, "emoji": "üò¢"},    # Bu·ªìn b√£: ti√™u c·ª±c, k√≠ch th√≠ch th·∫•p
    "Depressed": {"valence": -0.7, "arousal": -0.7, "emoji": "üòû"},    # Tr·∫ßm c·∫£m: ti√™u c·ª±c cao, k√≠ch th√≠ch r·∫•t th·∫•p
    "Bored":     {"valence": -0.3, "arousal": -0.7, "emoji": "üòí"},    # Ch√°n n·∫£n: ti√™u c·ª±c v·ª´a ph·∫£i, k√≠ch th√≠ch th·∫•p
    
    # V√πng ph·∫£i d∆∞·ªõi - c·∫£m x√∫c t√≠ch c·ª±c, k√≠ch th√≠ch th·∫•p
    "Relaxed":   {"valence": 0.6, "arousal": -0.5, "emoji": "üòå"},     # Th∆∞ gi√£n: t√≠ch c·ª±c, k√≠ch th√≠ch th·∫•p
    "Sleepy":    {"valence": 0.3, "arousal": -0.7, "emoji": "üò¥"},     # Bu·ªìn ng·ªß: t√≠ch c·ª±c v·ª´a ph·∫£i, k√≠ch th√≠ch r·∫•t th·∫•p
    "Calm":      {"valence": 0.4, "arousal": -0.4, "emoji": "üòä"},     # B√¨nh tƒ©nh: t√≠ch c·ª±c v·ª´a ph·∫£i, k√≠ch th√≠ch th·∫•p
    
    # C√°c v·ªã tr√≠ trung t√≠nh v√† b·ªï sung
    "Neutral":   {"valence": 0.0, "arousal": 0.0, "emoji": "üòê"},      # Trung t√≠nh: kh√¥ng t√≠ch c·ª±c/ti√™u c·ª±c, k√≠ch th√≠ch trung b√¨nh
    "Tired":     {"valence": -0.2, "arousal": -0.6, "emoji": "üò©"}     # M·ªát m·ªèi: h∆°i ti√™u c·ª±c, k√≠ch th√≠ch th·∫•p
}

# B·∫£n ƒë·ªì t√™n c·∫£m x√∫c t·ª´ m√¥ h√¨nh ƒë·ªÉ kh·ªõp v·ªõi emotion_map
emotion_label_map = {
    "Happy": "Happy",
    "Sad": "Sad",
    "Angry": "Angry", 
    "Fear": "Fear",
    "Surprise": "Surprise",
    "Disgust": "Disgust",
    "Neutral": "Neutral"
}

labels = list(emotion_map.keys())
face_detection = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
model = tf.keras.models.load_model('network-5Labels.h5', compile=False)
settings = {'scaleFactor': 1.3, 'minNeighbors': 5, 'minSize': (50, 50)}

# === Heatmap Plot ===




from matplotlib.colors import ListedColormap

from matplotlib.colors import ListedColormap

def plot_circumplex(emotion_log):
    fig, ax = plt.subplots(figsize=(6, 6), facecolor='black')
    plt.style.use("dark_background")
    ax.set_xlim(-1, 1)
    ax.set_ylim(-1, 1)
    ax.axhline(0, color='blue')
    ax.axvline(0, color='blue')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_aspect('equal')
    ax.set_title("Emotion Heatmap in Circumplex", color='white')
    ax.add_artist(plt.Circle((0, 0), 1.0, color='white', fill=False, lw=1))

    # ƒê·ªãnh nghƒ©a l·∫°i v·ªã tr√≠ c√°c c·∫£m x√∫c tr√™n kh√¥ng gian valence-arousal
    directions = {
        # Ph·∫ßn t∆∞ th·ª© nh·∫•t (tr√™n ph·∫£i)
        "Happy":    (0.8, 0.5),    # V·ªã tr√≠ h·ª£p l√Ω h∆°n cho Happy
        "Excited":  (0.7, 0.7),    # C·∫£m x√∫c ph·∫•n kh√≠ch
        "Surprise": (0.5, 0.8),    # B·∫•t ng·ªù (tr√™n ph·∫£i)
        
        # Ph·∫ßn t∆∞ th·ª© hai (tr√™n tr√°i)
        "Angry":    (-0.7, 0.7),   # Gi·∫≠n d·ªØ
        "Fear":     (-0.5, 0.8),   # S·ª£ h√£i
        "Disgust":  (-0.8, 0.3),   # Gh√™ t·ªüm (tr√™n tr√°i)
        
        # Ph·∫ßn t∆∞ th·ª© ba (d∆∞·ªõi tr√°i)
        "Sad":      (-0.8, -0.6),  # Bu·ªìn b√£ 
        "Depressed":(-0.7, -0.8),  # Tr·∫ßm c·∫£m
        "Negative": (-0.9, -0.2),  # Ti√™u c·ª±c (d∆∞·ªõi tr√°i)
        
        # Ph·∫ßn t∆∞ th·ª© t∆∞ (d∆∞·ªõi ph·∫£i)
        "Relaxed":  (0.7, -0.7),   # Th∆∞ gi√£n
        "Sleepy":   (0.5, -0.8),   # Bu·ªìn ng·ªß
        "Tired":    (0.3, -0.9),   # M·ªát m·ªèi
        
        # C√°c v·ªã tr√≠ tr√™n tr·ª•c
        "Positive": (0.9, 0.0),    # T√≠ch c·ª±c (ph·∫£i)
        "Neutral":  (0.0, 0.0),    # Trung t√≠nh (gi·ªØa)
        "Calming":  (0.0, -0.9),   # B√¨nh tƒ©nh (d∆∞·ªõi)
        "Exciting": (0.0, 0.9),    # H√†o h·ª©ng (tr√™n)
        
        # C√°c c·∫£m x√∫c kh√°c
        "Bored":    (-0.3, -0.9)   # Ch√°n n·∫£n (d∆∞·ªõi)
    }
    
    for label, (x, y) in directions.items():
        color = 'white'
        if "Negative" in label or "Sad" in label or "Angry" in label or "Fear" in label or "Disgust" in label:
            color = '#FF6B6B'
        elif "Positive" in label or "Happy" in label:
            color = '#90EE90'
        elif "Exciting" in label or "Excited" in label or "Surprise" in label:
            color = '#FFA07A'
        elif "Calming" in label or "Relaxed" in label or "Sleepy" in label:
            color = '#87CEFA'
        ax.text(x, y, label, ha='center', va='center', fontsize=9, color=color, fontweight='bold')

    if len(emotion_log) >= 2:
        # T·∫°o l∆∞·ªõi (grid) ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn ƒëi qua
        grid_size = 200 # TƒÉng ƒë·ªô ph√¢n gi·∫£i ƒë·ªÉ ƒë∆∞·ªùng ƒëi ch√≠nh x√°c h∆°n
        visit_count = np.zeros((grid_size, grid_size))
        color_mask = np.zeros((grid_size, grid_size, 4)) # RGBA, m·∫∑c ƒë·ªãnh trong su·ªët

        # Duy·ªát qua t·ª´ng ƒëo·∫°n ƒë∆∞·ªùng ƒëi
        for i in range(1, len(emotion_log)):
            x0, y0 = emotion_log[i - 1]["valence"], emotion_log[i - 1]["arousal"]
            x1, y1 = emotion_log[i]["valence"], emotion_log[i]["arousal"]
            
            # T√≠nh s·ªë ƒëi·ªÉm n·ªôi suy d·ª±a tr√™n kho·∫£ng c√°ch
            distance = np.sqrt((x1-x0)**2 + (y1-y0)**2)
            steps = max(5, int(distance * 150)) # Nhi·ªÅu ƒëi·ªÉm n·ªôi suy h∆°n ƒë·ªÉ m·ªãn
            
            # N·ªôi suy v√† c·∫≠p nh·∫≠t visit_count theo ki·ªÉu "b√∫t t√¥ m√†u"
            for alpha in np.linspace(0, 1, steps):
                x = x0 * (1 - alpha) + x1 * alpha
                y = y0 * (1 - alpha) + y1 * alpha
                
                # √Ånh x·∫° v√†o grid
                grid_x = int((x + 1) / 2 * (grid_size - 1))
                grid_y = int((y + 1) / 2 * (grid_size - 1))
                grid_x = np.clip(grid_x, 0, grid_size - 1)
                grid_y = np.clip(grid_y, 0, grid_size - 1)
                
                # TƒÉng visit_count cho √¥ ch√≠nh
                visit_count[grid_y, grid_x] += 1.0

                # L√†m d√†y ƒë∆∞·ªùng v·∫Ω m·ªôt c√°ch c√≥ ki·ªÉm so√°t (ch·ªâ c√°c √¥ li·ªÅn k·ªÅ)
                thickness_radius = 1 # Ch·ªâ t√¥ m√†u c√°c √¥ ngay s√°t c·∫°nh
                for dx in [-thickness_radius, 0, thickness_radius]:
                    for dy in [-thickness_radius, 0, thickness_radius]:
                        if dx == 0 and dy == 0: continue # B·ªè qua √¥ trung t√¢m
                        nx, ny = grid_x + dx, grid_y + dy
                        if 0 <= nx < grid_size and 0 <= ny < grid_size:
                            # Th√™m m·ªôt ph·∫ßn nh·ªè v√†o √¥ l√¢n c·∫≠n ƒë·ªÉ t·∫°o ƒë·ªô d√†y - Gi·∫£m xu·ªëng
                            visit_count[ny, nx] += 0.15 # Gi·∫£m t·ª´ 0.3 xu·ªëng 0.15

        # Chu·∫©n h√≥a v√† t·∫°o m√†u
        max_count = np.max(visit_count)
        if max_count > 0:
            # Ng∆∞·ª°ng ƒë·ªè t∆∞∆°ng ƒë·ªëi - Gi·∫£m xu·ªëng 15% c·ªßa max_count ƒë·ªÉ nh·∫°y h∆°n
            red_threshold = 0.15 * max_count + 1e-6 # Th√™m epsilon nh·ªè ƒë·ªÉ tr√°nh chia cho 0
            
            # H·ªá s·ªë ƒë·ªÉ ƒë·∫£m b·∫£o m√†u ƒë·∫°t ƒë·ªânh
            green_intensity_factor = 1.0 / red_threshold
            red_intensity_factor = 1.0 / max(1e-6, max_count - red_threshold)

            for i in range(grid_size):
                for j in range(grid_size):
                    count = visit_count[j, i] # visit_count d√πng [y, x]
                    if count > 0:
                        real_x = i / (grid_size - 1) * 2 - 1
                        real_y = j / (grid_size - 1) * 2 - 1
                        
                        if real_x**2 + real_y**2 <= 1:
                            # T√≠nh to√°n m√†u s·∫Øc
                            if count > red_threshold:
                                # Chuy·ªÉn sang ƒë·ªè
                                red_ratio = min(1.0, (count - red_threshold) * red_intensity_factor)
                                r = 0.3 + 0.6 * red_ratio # ƒê·ªè tƒÉng t·ª´ 0.3 ƒë·∫øn 0.9
                                g = 0.7 * (1 - red_ratio) # Xanh l√° gi·∫£m t·ª´ 0.7 xu·ªëng 0
                                b = 0.1
                            else:
                                # Gi·ªØ m√†u xanh l√°
                                green_ratio = count * green_intensity_factor
                                r = 0.1 + 0.2 * green_ratio # ƒê·ªè th·∫•p
                                g = 0.3 + 0.6 * green_ratio # Xanh l√° tƒÉng t·ª´ 0.3 ƒë·∫øn 0.9
                                b = 0.1
                            
                            # T√≠nh alpha (ƒë·ªô trong su·ªët) - C·ªê ƒê·ªäNH ƒë·ªÉ c∆∞·ªùng ƒë·ªô m√†u ƒë·ªìng ƒë·ªÅu
                            alpha = 0.85 # ƒê·∫∑t alpha c·ªë ƒë·ªãnh cho m·ªçi ƒëi·ªÉm ƒë∆∞·ª£c v·∫Ω
                            
                            color_mask[j, i] = [r, g, b, alpha]
            
            # L√†m m∆∞·ª£t heatmap b·∫±ng Gaussian Filter - Gi·∫£m sigma
            sigma_blur = 1.2 # Gi·∫£m t·ª´ 1.5 xu·ªëng 1.2
            for c in range(4):
                color_mask[:, :, c] = gaussian_filter(color_mask[:, :, c], sigma=sigma_blur)
            
            # Hi·ªÉn th·ªã heatmap
            ax.imshow(color_mask, extent=[-1, 1, -1, 1], origin='lower', interpolation='bilinear')

    # V·∫Ω v·ªã tr√≠ hi·ªán t·∫°i (ƒëi·ªÉm m√†u v√†ng)
    if len(emotion_log) > 0:
        last_entry = emotion_log[-1]
        last_x, last_y = last_entry["valence"], last_entry["arousal"]
        ax.plot(last_x, last_y, 'yo', markersize=8) # Gi·∫£m k√≠ch th∆∞·ªõc ƒëi·ªÉm v√†ng m·ªôt ch√∫t
        ax.text(0, -1.15, f"Valence: {last_x:.2f}   Arousal: {last_y:.2f}",
                ha='center', va='top', fontsize=10, color='white', fontweight='bold')
    else:
        # Khi ch∆∞a c√≥ d·ªØ li·ªáu, ch·ªâ hi·ªÉn th·ªã text ch·ªù
        ax.text(0, 0, "Waiting for data...", ha='center', va='center', color='gray', fontsize=10)
        ax.text(0, -1.15, f"Valence: 0.00   Arousal: 0.00",
                ha='center', va='top', fontsize=10, color='white', fontweight='bold')

    return fig

# === Emotion Data Management ===
def load_emotion_data():
    try:
        with open('emotion_log.json', 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {"emotions": {}, "metadata": {"last_updated": datetime.now().isoformat(), "version": "1.0"}}

def save_emotion_data(emotion_data):
    with open('emotion_log.json', 'w') as f:
        json.dump(emotion_data, f, indent=4)

def update_emotion_stats(emotion_log, current_emotion):
    emotion_data = load_emotion_data()
    
    if current_emotion not in emotion_data["emotions"]:
        emotion_data["emotions"][current_emotion] = {
            "count": 0,
            "duration": 0,
            "last_detected": None,
            "average_valence": 0,
            "average_arousal": 0
        }
    
    stats = emotion_data["emotions"][current_emotion]
    stats["count"] += 1
    stats["last_detected"] = datetime.now().isoformat()
    
    # Update averages
    if len(emotion_log) > 0:
        current_entry = emotion_log[-1]
        stats["average_valence"] = (stats["average_valence"] * (stats["count"] - 1) + current_entry["valence"]) / stats["count"]
        stats["average_arousal"] = (stats["average_arousal"] * (stats["count"] - 1) + current_entry["arousal"]) / stats["count"]
    
    emotion_data["metadata"]["last_updated"] = datetime.now().isoformat()
    save_emotion_data(emotion_data)

# === Real-time Video Capture and Emotion Detection ===
def process_stream(cap, custom_frame_skip):
    global prev_head_pose, prev_shoulders_pos, agitation_window
    
    # Reset ho√†n to√†n tr·∫°ng th√°i
    if 'emotion_log' in st.session_state:
        del st.session_state.emotion_log
    if 'smooth_points' in st.session_state:
        del st.session_state.smooth_points
    
    # Kh·ªüi t·∫°o l·∫°i t·ª´ ƒë·∫ßu
    st.session_state.emotion_log = []
    st.session_state.smooth_points = []
    st.session_state.last_save_time = time.time()
    st.session_state.prev_emotion = None
    st.session_state.first_detect = True

    emotion_log = st.session_state.emotion_log
    col1, col2 = st.columns([3, 2])
    frame_placeholder = col1.empty()
    chart_placeholder = col2.empty()
    status_placeholder = st.empty()
    stop_btn = st.button("\U0001F534 Stop")

    # Kh·ªüi t·∫°o bi·ªÉu ƒë·ªì tr·ªëng
    chart_placeholder.pyplot(plot_circumplex([]))

    # T·ªëi ∆∞u x·ª≠ l√Ω video
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)
    
    # T·∫°o worker threads
    frame_count = 0
    q_in, q_out = queue.Queue(maxsize=2), queue.Queue(maxsize=2)
    
    NUM_WORKERS = 2
    workers = []
    for _ in range(NUM_WORKERS):
        worker = threading.Thread(target=emotion_predictor_worker, args=(q_in, q_out), daemon=True)
        worker.start()
        workers.append(worker)
    
    last_chart_time = time.time()
    last_frame_time = time.time()
    fps_counter = 0
    fps = 0

    def update_fps():
        nonlocal fps_counter, fps, last_frame_time
        fps_counter += 1
        if time.time() - last_frame_time >= 1.0:
            fps = fps_counter
            fps_counter = 0
            last_frame_time = time.time()
            
    frame_buffer = None
    results_buffer = []
    
    # Kh·ªüi t·∫°o MediaPipe Holistic
    with mp_holistic.Holistic(
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as holistic:
        try:
            while cap.isOpened():
                if stop_btn:
                    if emotion_log:
                        update_emotion_stats(emotion_log, emotion_log[-1]["emotion"])
                        break

                ret, img = cap.read()
                if not ret:
                    break
                        
                update_fps()
                
                if img.shape[1] > 640:
                    scale_factor = 640 / img.shape[1]
                    img = cv2.resize(img, (0, 0), fx=scale_factor, fy=scale_factor)

                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                frame_count += 1
                
                # X·ª≠ l√Ω v·ªõi MediaPipe
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                results = holistic.process(img_rgb)
                
                # T√≠nh ƒëi·ªÉm k√≠ch ƒë·ªông
                agitation_score = calculate_agitation_score(
                    results,
                    prev_head_pose,
                    prev_shoulders_pos,
                    agitation_window
                )
                
                # C·∫≠p nh·∫≠t v·ªã tr√≠ tr∆∞·ªõc ƒë√≥
                if results.pose_landmarks:
                    prev_head_pose = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.NOSE]
                    left_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]
                    right_shoulder = results.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]
                    prev_shoulders_pos = (
                        (left_shoulder.x + right_shoulder.x) / 2,
                        (left_shoulder.y + right_shoulder.y) / 2
                    )
                
                # V·∫Ω landmarks
                if results.pose_landmarks:
                    mp_drawing.draw_landmarks(
                        img,
                        results.pose_landmarks,
                        mp_holistic.POSE_CONNECTIONS,
                        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()
                    )
                
                # Hi·ªÉn th·ªã ƒëi·ªÉm k√≠ch ƒë·ªông v√† th√¥ng tin debug
                debug_text = f"Agitation: {agitation_score:.2f}"
                if results.pose_landmarks:
                    debug_text += f" | Landmarks: {len(results.pose_landmarks.landmark)}"
                else:
                    debug_text += " | No landmarks detected"
                
                cv2.putText(img, debug_text, 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # Ch·ªâ g·ª≠i frame ƒë·ªÉ d·ª± ƒëo√°n m·ªói DETECT_EVERY_N_FRAMES
                if frame_count % (custom_frame_skip * DETECT_EVERY_N_FRAMES) == 0:
                    try:
                        q_in.put((img.copy(), gray), block=False)
                    except queue.Full:
                        pass

                # Ki·ªÉm tra v√† x·ª≠ l√Ω k·∫øt qu·∫£
                frame_to_show = img.copy()
                
                cv2.putText(frame_to_show, f"FPS: {fps}", (10, 20), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                if not q_out.empty():
                    frame, results = q_out.get()
                    frame_buffer = frame.copy()
                    results_buffer = results
                    
                    for res in results:
                        x, y, w, h = res['rect']
                        label = res['label']
                        val = res['valence']
                        aro = res['arousal']
                        conf = res['conf']
                        emoji = res['emoji']
                        
                        cv2.rectangle(frame_to_show, (x, y), (x+w, y+h), (0, 255, 0), 2)
                        cv2.rectangle(frame_to_show, (x, y-30), (x+w, y), (0, 0, 0, 0.7), -1)
                        label_text = f"{label} {emoji}"
                        cv2.putText(frame_to_show, label_text, (x+5, y-10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        
                        # Th√™m ƒëi·ªÉm k√≠ch ƒë·ªông v√†o log
                        current_time = datetime.now().isoformat()
                        emotion_log.append({
                            "timestamp": current_time,
                            "emotion": label,
                            "confidence": conf,
                            "valence": val,
                            "arousal": aro,
                            "agitation": agitation_score
                        })
                        
                        if len(emotion_log) > MAX_LOG_LENGTH:
                            del emotion_log[0]
                
                # Hi·ªÉn th·ªã frame
                frame_placeholder.image(cv2.cvtColor(frame_to_show, cv2.COLOR_BGR2RGB), channels="RGB")
                
                # C·∫≠p nh·∫≠t bi·ªÉu ƒë·ªì
                if time.time() - last_chart_time > 1.5:
                    if emotion_log:
                        chart_placeholder.pyplot(plot_circumplex(emotion_log))
                    last_chart_time = time.time()
                    
                status_placeholder.text(f"Processing... FPS: {fps}")
                time.sleep(0.001)
                
        finally:
            # D·ªçn d·∫πp t√†i nguy√™n
            for _ in range(NUM_WORKERS):
                try:
                    q_in.put((None, None), block=False)
                except queue.Full:
                    pass
            
            for worker in workers:
                worker.join(timeout=1.0)

            cap.release()
            cv2.destroyAllWindows()
            status_placeholder.text("Processing completed")

# === Threaded Prediction Worker ===
def emotion_predictor_worker(q_in, q_out):
    while True:
        try:
            frame, gray = q_in.get(timeout=3.0)
            if frame is None:
                break

            is_first_detection = 'first_detect' in st.session_state and st.session_state.first_detect

            scale_factor = 0.5
            small_gray = cv2.resize(gray, (0, 0), fx=scale_factor, fy=scale_factor)
            small_gray = cv2.equalizeHist(small_gray)

            if is_first_detection:
                scaleFactor = 1.08
                minNeighbors = 2
                minSize = (20, 20)
            else:
                scaleFactor = 1.1
                minNeighbors = 3
                minSize = (25, 25)

            faces = face_detection.detectMultiScale(
                small_gray, 
                scaleFactor=scaleFactor,
                minNeighbors=minNeighbors,
                minSize=minSize
            )

            logs = []
            for (x, y, w, h) in faces:
                x, y, w, h = int(x/scale_factor), int(y/scale_factor), int(w/scale_factor), int(h/scale_factor)

                padding = int(0.05 * w)
                x_padded = max(0, x - padding)
                y_padded = max(0, y - padding)
                w_padded = min(gray.shape[1] - x_padded, w + 2*padding)
                h_padded = min(gray.shape[0] - y_padded, h + 2*padding)

                face = gray[y_padded:y_padded+h_padded, x_padded:x_padded+w_padded]

                if face.size == 0 or face.shape[0] == 0 or face.shape[1] == 0:
                    continue

                try:
                    face_resized = cv2.resize(face, (48, 48))
                    face_normalized = face_resized / 255.0

                    prediction = model.predict(np.array([face_normalized.reshape((48, 48, 1))]), verbose=0)
                    label = labels[np.argmax(prediction)]
                    confidence = float(np.max(prediction))

                    if confidence > 0.3:
                        if is_first_detection:
                            st.session_state.first_detect = False

                        # Ph∆∞∆°ng ph√°p t√≠nh valence v√† arousal
                        # Ch·ªçn t·∫•t c·∫£ c·∫£m x√∫c c√≥ x√°c su·∫•t tr√™n ng∆∞·ª°ng ƒë·ªÉ c√≥ s·ª± pha tr·ªôn t·ªët h∆°n
                        threshold = 0.1  # Ch·ªâ x√©t c·∫£m x√∫c c√≥ x√°c su·∫•t t·ª´ 10% tr·ªü l√™n
                        significant_emotions = [(i, prob) for i, prob in enumerate(prediction[0]) if prob >= threshold]
                        
                        # N·∫øu kh√¥ng c√≥ c·∫£m x√∫c n√†o v∆∞·ª£t ng∆∞·ª°ng, l·∫•y c·∫£m x√∫c c√≥ x√°c su·∫•t cao nh·∫•t
                        if not significant_emotions:
                            significant_emotions = [(np.argmax(prediction[0]), np.max(prediction[0]))]
                        
                        # Chu·∫©n h√≥a l·∫°i tr·ªçng s·ªë ƒë·ªÉ t·ªïng b·∫±ng 1
                        total_weight = sum(prob for _, prob in significant_emotions)
                        normalized_emotions = [(i, prob/total_weight) for i, prob in significant_emotions]
                        
                        # T√≠nh valence v√† arousal d·ª±a tr√™n v·ªã tr√≠ trung b√¨nh c√≥ tr·ªçng s·ªë
                        valence = 0
                        arousal = 0
                        intensity = sum(prob for _, prob in normalized_emotions)
                        
                        for i, weight in normalized_emotions:
                            emotion_name = labels[i]
                            # √Ånh x·∫° t√™n c·∫£m x√∫c t·ª´ model sang emotion_map
                            mapped_emotion = emotion_label_map.get(emotion_name, "Neutral")
                            
                            # L·∫•y gi√° tr·ªã valence v√† arousal t·ª´ b·∫£n ƒë·ªì c·∫£m x√∫c - gi·ªØ nguy√™n gi√° tr·ªã
                            valence += emotion_map[mapped_emotion]['valence'] * weight
                            arousal += emotion_map[mapped_emotion]['arousal'] * weight
                        
                        # Th√™m nhi·ªÖu nh·ªè ƒë·ªÉ t·∫°o bi·∫øn ƒë·ªông nh·∫π
                        noise_factor = 0.02  # Gi·∫£m nhi·ªÖu xu·ªëng ƒë·ªÉ kh√¥ng l√†m m√©o m√≥ v·ªã tr√≠ qua m·ªói khung h√¨nh
                        valence += np.random.normal(0, noise_factor)
                        arousal += np.random.normal(0, noise_factor)
                        
                        # Ch·ªâ gi·ªõi h·∫°n trong kho·∫£ng [-1, 1] ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n c·ªßa bi·ªÉu ƒë·ªì
                        # Kh√¥ng √°p d·ª•ng h·ªá s·ªë scaling n√†o - gi·ªØ nguy√™n v·ªã tr√≠ theo t√≠nh to√°n
                        valence = np.clip(valence, -1, 1)
                        arousal = np.clip(arousal, -1, 1)
                        
                        # L√†m tr√≤n ƒë·∫øn 2 ch·ªØ s·ªë th·∫≠p ph√¢n
                        valence = round(valence, 2)
                        arousal = round(arousal, 2)

                        logs.append({
                            "label": label,
                            "conf": confidence,
                            "valence": valence,
                            "arousal": arousal,
                            "intensity": intensity,
                            "emoji": emotion_map[label]['emoji'],
                            "rect": (x, y, w, h)
                        })
                except Exception:
                    continue

            try:
                q_out.put((frame, logs), block=False)
            except queue.Full:
                pass

        except queue.Empty:
            continue
        except Exception:
            continue

def calculate_agitation_score(landmarks, prev_head_pose=None, prev_shoulders_pos=None, agitation_window=None):
    """
    T√≠nh to√°n ƒëi·ªÉm k√≠ch ƒë·ªông d·ª±a tr√™n chuy·ªÉn ƒë·ªông c·ªßa ƒë·∫ßu v√† vai s·ª≠ d·ª•ng MediaPipe Holistic
    """
    if agitation_window is None:
        agitation_window = []
        
    if landmarks.pose_landmarks:
        # L·∫•y c√°c ƒëi·ªÉm m·ªëc quan tr·ªçng
        head_pose = landmarks.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.NOSE]
        left_shoulder = landmarks.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks.pose_landmarks.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER]
        
        # T√≠nh v·ªã tr√≠ trung b√¨nh c·ªßa vai
        shoulders_pos = (
            (left_shoulder.x + right_shoulder.x) / 2,
            (left_shoulder.y + right_shoulder.y) / 2
        )
        
        # T√≠nh chuy·ªÉn ƒë·ªông c·ªßa ƒë·∫ßu
        if prev_head_pose is not None:
            head_movement = math.sqrt(
                (head_pose.x - prev_head_pose.x) ** 2 +
                (head_pose.y - prev_head_pose.y) ** 2 +
                (head_pose.z - prev_head_pose.z) ** 2
            )
        else:
            head_movement = 0
            
        # T√≠nh chuy·ªÉn ƒë·ªông c·ªßa vai
        if prev_shoulders_pos is not None:
            shoulder_movement = math.sqrt(
                (shoulders_pos[0] - prev_shoulders_pos[0]) ** 2 +
                (shoulders_pos[1] - prev_shoulders_pos[1]) ** 2
            )
        else:
            shoulder_movement = 0
            
        # T√≠nh ƒëi·ªÉm k√≠ch ƒë·ªông d·ª±a tr√™n chuy·ªÉn ƒë·ªông
        # TƒÉng ƒë·ªô nh·∫°y v·ªõi chuy·ªÉn ƒë·ªông ƒë·∫ßu (nh√¢n 2)
        agitation_score = (head_movement * 2 + shoulder_movement) / 3
        
        # Th√™m ƒëi·ªÉm v√†o c·ª≠a s·ªï
        agitation_window.append(agitation_score)
        
        # Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc c·ª≠a s·ªï (30 frames)
        if len(agitation_window) > 30:
            agitation_window.pop(0)
            
        # T√≠nh ƒëi·ªÉm k√≠ch ƒë·ªông trung b√¨nh
        avg_agitation = sum(agitation_window) / len(agitation_window)
        
        # Chu·∫©n h√≥a v·ªÅ kho·∫£ng [0, 1]
        return min(max(avg_agitation, 0), 1)
        
    return 0

# === Streamlit UI ===
st.set_page_config(page_title="Facial Emotion Heatmap", layout="wide")
st.title("üé≠ Real-time Facial Emotion Recognition with Heatmap")

mode = st.radio("Select Mode:", ["Use Webcam", "Upload Video", "Test Video"])

if mode == "Use Webcam":
    if st.button("Start Webcam Analysis"):
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        process_stream(cap, FRAME_SKIP)

elif mode == "Upload Video":
    video_file = st.file_uploader("Upload your .mp4 video", type=["mp4"])
    if video_file:
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(video_file.read())
        st.video(tfile.name)
        if st.button("Analyze Uploaded Video"):
            cap = cv2.VideoCapture(tfile.name)
            process_stream(cap, FRAME_SKIP)

elif mode == "Test Video":
    st.subheader("Test Video Settings")
    
    # Video selection
    test_videos = ["test_video1.mp4", "test_video2.mp4", "emotions_test.mp4"]
    
    # Ki·ªÉm tra xem video n√†o t·ªìn t·∫°i
    existing_videos = []
    for video in test_videos:
        if os.path.exists(video):
            existing_videos.append(video)
    
    # N·∫øu kh√¥ng t√¨m th·∫•y video n√†o, hi·ªÉn th·ªã th√¥ng b√°o
    if not existing_videos:
        st.warning("No test videos found. Please place test videos in the same directory.")
        
        # Cung c·∫•p t√πy ch·ªçn ƒë·ªÉ t·∫£i l√™n video test
        custom_test = st.file_uploader("Upload a test video", type=["mp4"])
        if custom_test:
            tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
            tfile.write(custom_test.read())
            custom_path = tfile.name
            st.success(f"Test video uploaded successfully: {os.path.basename(custom_path)}")
            existing_videos = [custom_path]
    
    if existing_videos:
        selected_video = st.selectbox("Select a test video", existing_videos)
        
        # C√†i ƒë·∫∑t ph√°t video
        col1, col2 = st.columns(2)
        with col1:
            speed = st.slider("Playback Speed", min_value=0.25, max_value=2.0, value=1.0, step=0.25)
            frame_skip = st.slider("Frame Skip", min_value=1, max_value=10, value=3, step=1)
        
        with col2:
            path_thickness = st.slider("Path Thickness", min_value=0.3, max_value=0.6, value=0.45, step=0.05)
            smoothness = st.slider("Path Smoothness", min_value=1.0, max_value=2.0, value=1.4, step=0.1)
        
        # Hi·ªÉn th·ªã preview v√† n√∫t ch·∫°y
        st.video(selected_video)
        
        if st.button("Run Test Analysis"):
            # √Åp d·ª•ng c√°c thi·∫øt l·∫≠p t·ª´ ng∆∞·ªùi d√πng
            # S·ª≠ d·ª•ng bi·∫øn t·ª´ slider m√† kh√¥ng c·∫ßn bi·∫øn global
            custom_frame_skip = frame_skip
            
            # Ti·ªán √≠ch ƒë·ªÉ thi·∫øt l·∫≠p c√°c th√¥ng s·ªë cho path visualization
            st.session_state.path_thickness = path_thickness
            st.session_state.path_smoothness = smoothness
            
            # M·ªü v√† x·ª≠ l√Ω video
            cap = cv2.VideoCapture(selected_video)
            
            # ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô ph√°t video
            if speed != 1.0:
                original_fps = cap.get(cv2.CAP_PROP_FPS)
                st.info(f"Original video FPS: {original_fps:.1f}, Adjusted FPS: {original_fps * speed:.1f}")
            
            # Truy·ªÅn frame_skip d∆∞·ªõi d·∫°ng tham s·ªë cho process_stream
            process_stream(cap, custom_frame_skip)